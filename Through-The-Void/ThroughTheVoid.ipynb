{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sEmbxyD4rEFS"
      },
      "source": [
        "# Through the void"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C42y6wqGrM6c",
        "outputId": "e371c031-3a5d-45c8-eec1-f1a619eff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0a6521f9f0>"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBYVWGlerX8V"
      },
      "source": [
        "*загружаем дата-сет*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFi8s7xorbnm"
      },
      "outputs": [],
      "source": [
        "dataroot = '/content/img/'\n",
        "workers = 2\n",
        "batch_size = 128\n",
        "image_size = 64\n",
        "nc = 3\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "num_epochs = 5\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ind2A6TF3MVq"
      },
      "outputs": [],
      "source": [
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "train_data = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "val_data = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI_JKHnPzcbu"
      },
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "    \n",
        "        \n",
        "    def __init__(self, encoder, decoder, ngpu):\n",
        "        super(AE, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, x):\n",
        "    \n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class encoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(encoder, self).__init__()\n",
        "        \n",
        "        self.enc = nn.Sequential(\n",
        "          # input is Z, going into a convolution\n",
        "            nn.Conv2d( 3, ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.Conv2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.Conv2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.enc(x)\n",
        "    \n",
        "class decoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(decoder, self).__init__()\n",
        "        \n",
        "        self.drop_layer = nn.Dropout(p=0.001)\n",
        "        self.dec =nn.Sequential(\n",
        "        nn.ConvTranspose2d(ngf * 2, ngf * 4,4, 2, 1, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(ngf * 4, ngf * 8,4, 2, 1, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(ngf * 8, 3,4, 2, 1, bias=False),\n",
        "        nn.Tanh()\n",
        "                        )  \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.drop_layer(self.dec(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXUIuD5zAjMS"
      },
      "outputs": [],
      "source": [
        "manualSeed = random.randint(1, 10000)\n",
        "net = AE(encoder(), decoder(),ngpu).to(device)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    net = nn.DataParallel(net, list(range(ngpu)))\n",
        "  \n",
        "# optimizer.param_groups[0]['params'][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAqCYW_fA5Dl"
      },
      "outputs": [],
      "source": [
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(20, 60), gamma=1/2)\n",
        "img_list=[]\n",
        "\n",
        "l_1 = list(net.parameters()).copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9M6ckl1DOPa"
      },
      "outputs": [],
      "source": [
        "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
        "    \n",
        "    freq = max(epochs//20,1)\n",
        "    net.to(device)\n",
        "    \n",
        "    iters=0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        net.train()\n",
        "\n",
        "        losses_train = []\n",
        "        for X, _ in train_loader:\n",
        "            # Performing one step of minibatch stochastic gradient descent\n",
        "            \n",
        "            X = X.to(device)\n",
        "            out = net(X) \n",
        "            loss = criterion(X,out)       \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses_train.append(loss.item())\n",
        "\n",
        "            if (iters % 500 == 0) or ((epoch == num_epochs-1)):\n",
        "            \n",
        "              img_list.append((vutils.make_grid(out, padding=2, normalize=True)).cpu())\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "        \n",
        "        net.eval()\n",
        "        for X, _ in val_loader:\n",
        "            losses_val = []\n",
        "            \n",
        "            X = X.to(device)\n",
        "            out = net(X)\n",
        "            loss_val = criterion(X, out)\n",
        "            losses_val.append(loss_val.item())\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        if verbose and epoch%freq==0:\n",
        "            mean_val = sum(losses_val)/len(losses_val)\n",
        "            mean_train = sum(losses_train)/len(losses_train)\n",
        "\n",
        "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'\\\n",
        "                  .format(epoch, epochs, mean_train, mean_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siln6w29DtVx",
        "outputId": "1e123e03-eb1e-4c25-8e5b-83e8a777a127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 || Loss:  Train 0.4537 | Validation 0.3896\n",
            "Epoch 2/5 || Loss:  Train 0.3176 | Validation 0.2935\n",
            "Epoch 3/5 || Loss:  Train 0.2479 | Validation 0.2970\n",
            "Epoch 4/5 || Loss:  Train 0.2099 | Validation 0.1977\n",
            "Epoch 5/5 || Loss:  Train 0.1846 | Validation 0.1825\n"
          ]
        }
      ],
      "source": [
        "train(5, net, criterion, optimizer, train_data, val_data, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTbp25kdv-26",
        "outputId": "afa037e2-0111-4be2-d615-aec213043ad0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True]],\n",
              "\n",
              "        [[True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True]],\n",
              "\n",
              "        [[True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True],\n",
              "         [True, True, True, True]]], device='cuda:0')"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l_2 = list(net.parameters()).copy()\n",
        "l_1[0][1] == l_2[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHHE8Kd3I96c",
        "outputId": "2cf33367-38f5-404b-ede9-f6334c659a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 || Loss:  Train 0.3570 | Validation 0.3503\n",
            "Epoch 2/10 || Loss:  Train 0.3600 | Validation 0.3503\n",
            "Epoch 3/10 || Loss:  Train 0.3557 | Validation 0.3504\n",
            "Epoch 4/10 || Loss:  Train 0.3546 | Validation 0.3504\n",
            "Epoch 5/10 || Loss:  Train 0.3550 | Validation 0.3505\n",
            "Epoch 6/10 || Loss:  Train 0.3560 | Validation 0.3506\n",
            "Epoch 7/10 || Loss:  Train 0.3522 | Validation 0.3508\n",
            "Epoch 8/10 || Loss:  Train 0.3543 | Validation 0.3510\n",
            "Epoch 9/10 || Loss:  Train 0.3559 | Validation 0.3513\n",
            "Epoch 10/10 || Loss:  Train 0.3567 | Validation 0.3515\n"
          ]
        }
      ],
      "source": [
        "param_1 = optimizer.param_groups[0]['params'][0] # это веса == list(net.parameters())\n",
        "\n",
        "manualSeed = random.randint(1, 10000)\n",
        "net = AE(encoder(), decoder(),ngpu).to(device)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "img_list=[]\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "train(10, net, criterion, optimizer, train_data, val_data, scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78LABU_eyVj"
      },
      "source": [
        "# Laplacian Pyramid loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuThc4rp4H0H"
      },
      "outputs": [],
      "source": [
        "def gauss_kernel(size=5, device=torch.device('cpu'), channels=3):\n",
        "    kernel = torch.tensor([[1., 4., 6., 4., 1],\n",
        "                           [4., 16., 24., 16., 4.],\n",
        "                           [6., 24., 36., 24., 6.],\n",
        "                           [4., 16., 24., 16., 4.],\n",
        "                           [1., 4., 6., 4., 1.]])\n",
        "    kernel /= 256.\n",
        "    kernel = kernel.repeat(channels, 1, 1, 1)\n",
        "    kernel = kernel.to(device)\n",
        "    return kernel\n",
        "\n",
        "def downsample(x):\n",
        "    return x[:, :, ::2, ::2]\n",
        "\n",
        "def upsample(x):\n",
        "    cc = torch.cat([x, torch.zeros(x.shape[0], x.shape[1], x.shape[2], x.shape[3], device=x.device)], dim=3)\n",
        "    cc = cc.view(x.shape[0], x.shape[1], x.shape[2]*2, x.shape[3])\n",
        "    cc = cc.permute(0,1,3,2)\n",
        "    cc = torch.cat([cc, torch.zeros(x.shape[0], x.shape[1], x.shape[3], x.shape[2]*2, device=x.device)], dim=3)\n",
        "    cc = cc.view(x.shape[0], x.shape[1], x.shape[3]*2, x.shape[2]*2)\n",
        "    x_up = cc.permute(0,1,3,2)\n",
        "    return conv_gauss(x_up, 4*gauss_kernel(channels=x.shape[1], device=x.device))\n",
        "\n",
        "def conv_gauss(img, kernel):\n",
        "    img = torch.nn.functional.pad(img, (2, 2, 2, 2), mode='reflect')\n",
        "    out = torch.nn.functional.conv2d(img, kernel, groups=img.shape[1])\n",
        "    return out\n",
        "\n",
        "def laplacian_pyramid(img, kernel, max_levels=3):\n",
        "    current = img\n",
        "    pyr = []\n",
        "    for level in range(max_levels):\n",
        "        filtered = conv_gauss(current, kernel)\n",
        "        down = downsample(filtered)\n",
        "        up = upsample(down)\n",
        "        diff = current-up\n",
        "        pyr.append(diff)\n",
        "        current = down\n",
        "    return pyr\n",
        "\n",
        "class LapLoss(torch.nn.Module):\n",
        "    def __init__(self, max_levels=3, channels=3, device=torch.device('cuda')):\n",
        "        super(LapLoss, self).__init__()\n",
        "        self.max_levels = max_levels\n",
        "        self.gauss_kernel = gauss_kernel(channels=channels, device=device)\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        pyr_input  = laplacian_pyramid(img=input, kernel=self.gauss_kernel, max_levels=self.max_levels)\n",
        "        pyr_target = laplacian_pyramid(img=target, kernel=self.gauss_kernel, max_levels=self.max_levels)\n",
        "        return sum(torch.nn.functional.l1_loss(a, b) for a, b in zip(pyr_input, pyr_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nIssXq_OZJi",
        "outputId": "4f47ce36-99a4-46df-c5b0-5aa452cdec98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 || Loss:  Train 0.3070 | Validation 0.2929\n",
            "Epoch 2/10 || Loss:  Train 0.2907 | Validation 0.2902\n",
            "Epoch 3/10 || Loss:  Train 0.2765 | Validation 0.2715\n",
            "Epoch 4/10 || Loss:  Train 0.2322 | Validation 0.2148\n",
            "Epoch 5/10 || Loss:  Train 0.2004 | Validation 0.1915\n",
            "Epoch 6/10 || Loss:  Train 0.1820 | Validation 0.1848\n",
            "Epoch 7/10 || Loss:  Train 0.1688 | Validation 0.1707\n",
            "Epoch 8/10 || Loss:  Train 0.1570 | Validation 0.1609\n",
            "Epoch 9/10 || Loss:  Train 0.1490 | Validation 0.1488\n",
            "Epoch 10/10 || Loss:  Train 0.1402 | Validation 0.1375\n"
          ]
        }
      ],
      "source": [
        "manualSeed = random.randint(1, 10000)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "net = AE(encoder(), decoder(),ngpu).to(device)\n",
        "\n",
        "criterion = LapLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002) \n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(20, 60), gamma=0.5)\n",
        "img_list=[]\n",
        "train(10, net, criterion, optimizer, train_data, val_data, scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNUZPAKkdZsi"
      },
      "source": [
        "## Finding  $\\; \\omega_{i1}, \\; \\omega_{i2} \\;$ from different random starts and save\n",
        "итог лежит в param_list_end: \n",
        "\n",
        "param_list_end[0] = $\\omega_{i1}$\n",
        "\n",
        "param_list_end[1] = $\\omega_{i2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1dTCbJ_Zhpe",
        "outputId": "6463bda6-a6fe-4ea3-a8d2-0cbd6943ff38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 || Loss:  Train 0.4610 | Validation 0.3914\n",
            "Epoch 2/10 || Loss:  Train 0.3281 | Validation 0.3092\n",
            "Epoch 3/10 || Loss:  Train 0.2588 | Validation 0.3121\n",
            "Epoch 4/10 || Loss:  Train 0.2213 | Validation 0.2120\n",
            "Epoch 5/10 || Loss:  Train 0.1940 | Validation 0.1880\n",
            "Epoch 6/10 || Loss:  Train 0.1805 | Validation 0.1787\n",
            "Epoch 7/10 || Loss:  Train 0.1761 | Validation 0.1701\n",
            "Epoch 8/10 || Loss:  Train 0.1577 | Validation 0.1539\n",
            "Epoch 9/10 || Loss:  Train 0.1536 | Validation 0.1558\n",
            "Epoch 10/10 || Loss:  Train 0.1435 | Validation 0.1421\n",
            "True\n",
            "Epoch 1/10 || Loss:  Train 0.4726 | Validation 0.4245\n",
            "Epoch 2/10 || Loss:  Train 0.3036 | Validation 0.2787\n",
            "Epoch 3/10 || Loss:  Train 0.2440 | Validation 0.2435\n",
            "Epoch 4/10 || Loss:  Train 0.2080 | Validation 0.2033\n",
            "Epoch 5/10 || Loss:  Train 0.1860 | Validation 0.2050\n",
            "Epoch 6/10 || Loss:  Train 0.1745 | Validation 0.1730\n",
            "Epoch 7/10 || Loss:  Train 0.1610 | Validation 0.1593\n",
            "Epoch 8/10 || Loss:  Train 0.1507 | Validation 0.1592\n",
            "Epoch 9/10 || Loss:  Train 0.1418 | Validation 0.1453\n",
            "Epoch 10/10 || Loss:  Train 0.1378 | Validation 0.1391\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "param_list_begin = [] # веса до обучения, это для проверки\n",
        "param_list_end = [] # веса после обучения, это для проверки\n",
        "\n",
        "for _ in range(2):\n",
        "  manualSeed = random.randint(1, 10000)\n",
        "  random.seed(manualSeed)\n",
        "  torch.manual_seed(manualSeed)\n",
        "\n",
        "  net = AE(encoder(), decoder(),ngpu).to(device)\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.0002) \n",
        "  scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(20, 60), gamma=0.5)\n",
        "  l_1 = list(net.parameters()).copy()\n",
        "  param_list_begin.append(l_1)\n",
        "  img_list=[]\n",
        "\n",
        "  train(10, net, criterion, optimizer, train_data, val_data, scheduler)\n",
        "\n",
        "  l_2 = list(net.parameters()).copy()\n",
        "  param_list_end.append(l_2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gw_UVLbGKrL",
        "outputId": "71ad8065-7247-4cfd-e34b-43bb18869edf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False],\n",
              "         [False, False, False, False]]], device='cuda:0')"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param_list_begin[0][0][0] == param_list_begin[1][0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWzK__YZQHXU"
      },
      "source": [
        "# Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A09qjDclIOnz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, Parameter\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy.special import binom\n",
        "\n",
        "\n",
        "class Bezier(Module):\n",
        "    def __init__(self, num_bends):\n",
        "        super(Bezier, self).__init__()\n",
        "        self.register_buffer(\n",
        "            'binom',\n",
        "            torch.Tensor(binom(num_bends - 1, np.arange(num_bends), dtype=np.float32))\n",
        "        )\n",
        "        self.register_buffer('range', torch.arange(0, float(num_bends)))\n",
        "        self.register_buffer('rev_range', torch.arange(float(num_bends - 1), -1, -1))\n",
        "\n",
        "    def forward(self, t):\n",
        "        return self.binom * \\\n",
        "               torch.pow(t, self.range) * \\\n",
        "               torch.pow((1.0 - t), self.rev_range)\n",
        "\n",
        "\n",
        "class PolyChain(Module):\n",
        "    def __init__(self, num_bends):\n",
        "        super(PolyChain, self).__init__()\n",
        "        self.num_bends = num_bends\n",
        "        self.register_buffer('range', torch.arange(0, float(num_bends)))\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_n = t * (self.num_bends - 1)\n",
        "        return torch.max(self.range.new([0.0]), 1.0 - torch.abs(t_n - self.range))\n",
        "\n",
        "\n",
        "class CurveModule(Module):\n",
        "\n",
        "    def __init__(self, fix_points, parameter_names=()):\n",
        "        super(CurveModule, self).__init__()\n",
        "        self.fix_points = fix_points\n",
        "        self.num_bends = len(self.fix_points)\n",
        "        self.parameter_names = parameter_names\n",
        "        self.l2 = 0.0\n",
        "\n",
        "    def compute_weights_t(self, coeffs_t):\n",
        "        w_t = [None] * len(self.parameter_names)\n",
        "        self.l2 = 0.0\n",
        "        for i, parameter_name in enumerate(self.parameter_names):\n",
        "            for j, coeff in enumerate(coeffs_t):\n",
        "                parameter = getattr(self, '%s_%d' % (parameter_name, j))\n",
        "                if parameter is not None:\n",
        "                    if w_t[i] is None:\n",
        "                        w_t[i] = parameter * coeff\n",
        "                    else:\n",
        "                        w_t[i] += parameter * coeff\n",
        "            if w_t[i] is not None:\n",
        "                self.l2 += torch.sum(w_t[i] ** 2)\n",
        "        return w_t\n",
        "\n",
        "\n",
        "class Linear(CurveModule):\n",
        "\n",
        "    def __init__(self, in_features, out_features, fix_points, bias=True):\n",
        "        super(Linear, self).__init__(fix_points, ('weight', 'bias'))\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.l2 = 0.0\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            self.register_parameter(\n",
        "                'weight_%d' % i,\n",
        "                Parameter(torch.Tensor(out_features, in_features), requires_grad=not fixed)\n",
        "            )\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            if bias:\n",
        "                self.register_parameter(\n",
        "                    'bias_%d' % i,\n",
        "                    Parameter(torch.Tensor(out_features), requires_grad=not fixed)\n",
        "                )\n",
        "            else:\n",
        "                self.register_parameter('bias_%d' % i, None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.in_features)\n",
        "        for i in range(self.num_bends):\n",
        "            getattr(self, 'weight_%d' % i).data.uniform_(-stdv, stdv)\n",
        "            bias = getattr(self, 'bias_%d' % i)\n",
        "            if bias is not None:\n",
        "                bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, coeffs_t):\n",
        "        weight_t, bias_t = self.compute_weights_t(coeffs_t)\n",
        "        return F.linear(input, weight_t, bias_t)\n",
        "\n",
        "\n",
        "class Conv2d(CurveModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, fix_points, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(Conv2d, self).__init__(fix_points, ('weight', 'bias'))\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            self.register_parameter(\n",
        "                'weight_%d' % i,\n",
        "                Parameter(\n",
        "                    torch.Tensor(out_channels, in_channels // groups, *kernel_size),\n",
        "                    requires_grad=not fixed\n",
        "                )\n",
        "            )\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            if bias:\n",
        "                self.register_parameter(\n",
        "                    'bias_%d' % i,\n",
        "                    Parameter(torch.Tensor(out_channels), requires_grad=not fixed)\n",
        "                )\n",
        "            else:\n",
        "                self.register_parameter('bias_%d' % i, None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        n = self.in_channels\n",
        "        for k in self.kernel_size:\n",
        "            n *= k\n",
        "        stdv = 1. / math.sqrt(n)\n",
        "        for i in range(self.num_bends):\n",
        "            getattr(self, 'weight_%d' % i).data.uniform_(-stdv, stdv)\n",
        "            bias = getattr(self, 'bias_%d' % i)\n",
        "            if bias is not None:\n",
        "                bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, coeffs_t):\n",
        "        weight_t, bias_t = self.compute_weights_t(coeffs_t)\n",
        "        return F.conv2d(input, weight_t, bias_t, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class _BatchNorm(CurveModule):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_features, fix_points, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True):\n",
        "        super(_BatchNorm, self).__init__(fix_points, ('weight', 'bias'))\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "\n",
        "        self.l2 = 0.0\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            if self.affine:\n",
        "                self.register_parameter(\n",
        "                    'weight_%d' % i,\n",
        "                    Parameter(torch.Tensor(num_features), requires_grad=not fixed)\n",
        "                )\n",
        "            else:\n",
        "                self.register_parameter('weight_%d' % i, None)\n",
        "        for i, fixed in enumerate(self.fix_points):\n",
        "            if self.affine:\n",
        "                self.register_parameter(\n",
        "                    'bias_%d' % i,\n",
        "                    Parameter(torch.Tensor(num_features), requires_grad=not fixed)\n",
        "                )\n",
        "            else:\n",
        "                self.register_parameter('bias_%d' % i, None)\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "            self.register_buffer('running_var', torch.ones(num_features))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "            self.register_parameter('num_batches_tracked', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "            self.num_batches_tracked.zero_()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            for i in range(self.num_bends):\n",
        "                getattr(self, 'weight_%d' % i).data.uniform_()\n",
        "                getattr(self, 'bias_%d' % i).data.zero_()\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, input, coeffs_t):\n",
        "        self._check_input_dim(input)\n",
        "\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            self.num_batches_tracked += 1\n",
        "            if self.momentum is None: \n",
        "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
        "            else: \n",
        "                exponential_average_factor = self.momentum\n",
        "        weight_t, bias_t = self.compute_weights_t(coeffs_t)\n",
        "        return F.batch_norm(\n",
        "            input, self.running_mean, self.running_var, weight_t, bias_t,\n",
        "            self.training or not self.track_running_stats,\n",
        "            exponential_average_factor, self.eps)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        version = metadata.get('version', None)\n",
        "\n",
        "        if (version is None or version < 2) and self.track_running_stats:\n",
        "            num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "            if num_batches_tracked_key not in state_dict:\n",
        "                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "        super(_BatchNorm, self)._load_from_state_dict(\n",
        "            state_dict, prefix, metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "\n",
        "class BatchNorm2d(_BatchNorm):\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 4:\n",
        "            raise ValueError('expected 4D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "\n",
        "\n",
        "class CurveNet(Module):\n",
        "    def __init__(self, num_classes, curve, architecture, num_bends, fix_start=True, fix_end=True,\n",
        "                 architecture_kwargs={}):\n",
        "        super(CurveNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_bends = num_bends\n",
        "        self.fix_points = [fix_start] + [False] * (self.num_bends - 2) + [fix_end]\n",
        "        \n",
        "        self.curve = curve\n",
        "        self.architecture = architecture\n",
        "\n",
        "        self.l2 = 0.0\n",
        "        self.coeff_layer = self.curve(self.num_bends)\n",
        "        self.net = self.architecture(num_classes, fix_points=self.fix_points, **architecture_kwargs)\n",
        "        self.curve_modules = []\n",
        "        for module in self.net.modules():\n",
        "            if issubclass(module.__class__, CurveModule):\n",
        "                self.curve_modules.append(module)\n",
        "\n",
        "    def import_base_parameters(self, base_model, index):\n",
        "        parameters = list(self.net.parameters())[index::self.num_bends]\n",
        "        base_parameters = base_model.parameters()\n",
        "        for parameter, base_parameter in zip(parameters, base_parameters):\n",
        "            parameter.data.copy_(base_parameter.data)\n",
        "\n",
        "    def import_base_buffers(self, base_model):\n",
        "        for buffer, base_buffer in zip(self.net._all_buffers(), base_model._all_buffers()):\n",
        "            buffer.data.copy_(base_buffer.data)\n",
        "\n",
        "    def export_base_parameters(self, base_model, index):\n",
        "        parameters = list(self.net.parameters())[index::self.num_bends]\n",
        "        base_parameters = base_model.parameters()\n",
        "        for parameter, base_parameter in zip(parameters, base_parameters):\n",
        "            base_parameter.data.copy_(parameter.data)\n",
        "\n",
        "    def init_linear(self):\n",
        "        parameters = list(self.net.parameters())\n",
        "        for i in range(0, len(parameters), self.num_bends):\n",
        "            weights = parameters[i:i+self.num_bends]\n",
        "            for j in range(1, self.num_bends - 1):\n",
        "                alpha = j * 1.0 / (self.num_bends - 1)\n",
        "                weights[j].data.copy_(alpha * weights[-1].data + (1.0 - alpha) * weights[0].data)\n",
        "\n",
        "    def weights(self, t):\n",
        "        coeffs_t = self.coeff_layer(t)\n",
        "        weights = []\n",
        "        for module in self.curve_modules:\n",
        "            weights.extend([w for w in module.compute_weights_t(coeffs_t) if w is not None])\n",
        "        return np.concatenate([w.detach().cpu().numpy().ravel() for w in weights])\n",
        "\n",
        "    def _compute_l2(self):\n",
        "        self.l2 = sum(module.l2 for module in self.curve_modules)\n",
        "\n",
        "    def forward(self, input, t=None):\n",
        "        if t is None:\n",
        "            t = input.data.new(1).uniform_()\n",
        "        coeffs_t = self.coeff_layer(t)\n",
        "        output = self.net(input, coeffs_t)\n",
        "        self._compute_l2()\n",
        "        return output\n",
        "\n",
        "\n",
        "def l2_regularizer(weight_decay):\n",
        "    return lambda model: 0.5 * weight_decay * model.l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE7bpva0Mmlg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
